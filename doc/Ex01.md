# HPC exercise 01
###### tags `uni-rostock` `HPC`

## Task 1.1: Systems share of supercomputer architectures
**Question**

How are the architectures represented in the top group of high-performance computing systems?

**Answer**

<img src="https://expertiza.csc.ncsu.edu/images/0/0c/Architecture_Share1.png" width="300">

> source: [https://expertiza.csc.ncsu.edu/](https://expertiza.csc.ncsu.edu/index.php/CSC/ECE_506_Spring_2013/1a_pg)

As the image shows, recent types of supercomputer are [massively parallel processing (MPP)](https://en.wikipedia.org/wiki/Massively_parallel) and [cluster](https://en.wikipedia.org/wiki/Computer_cluster).

**Difference between MPP and cluster**

In a cluster, each machine is largely independent of the others in terms of memory, disk, etc. They are interconnected using some variation on normal networking. The cluster exists mostly in the mind of the programmer and how s/he chooses to distribute the work.

In a Massively Parallel Processor, there really is only one machine with thousands of CPUs tightly interconnected. MPPs have exotic memory architectures to allow extremely high speed exchange of intermediate results with neighboring processors.

The major variants are SIMD (Single Instruction, Multiple Data) and MIMD (Multiple Instruction, Multiple Data). In a SIMD system, every processor is executing the same instruction at the same time, only on different bits of memory. Essentially, there is only one Program Counter. In a MIMD machine, each CPU has it's own PC.

MPPs can be a bitch to program and are of use only on algorithms that are embarrassingly parallel (that's actually what they call it). However, if you have such a problem, then an MPP can be shockingly fast. They are also incredibly expensive.
> credit: [Peter Rowell, MatthewMartin](https://stackoverflow.com/questions/5570936/what-is-the-difference-between-a-cluster-and-mpp-supercomputer-architecture)

## Task 1.2: Performance measures for parallel work
**Question**

Name and explain the parameters for the performance gain through parallelization.

**Answer:**

For an parallel program, the speed up can can defined as:
<img src="https://render.githubusercontent.com/render/math?math=S_n = T_1 / T_n">
* <img src="https://render.githubusercontent.com/render/math?math=S_n">: Speed up
* <img src="https://render.githubusercontent.com/render/math?math=T_1">: Time consume using 1 processor
* <img src="https://render.githubusercontent.com/render/math?math=T_n">: Time consume using n processor

Theoritically <img src="https://render.githubusercontent.com/render/math?math=S_n < n">, and <img src="https://render.githubusercontent.com/render/math?math=S_n"> decays when <img src="https://render.githubusercontent.com/render/math?math=n"> goes higher. However, sometimes the effect of memory hierarchy makes <img src="https://render.githubusercontent.com/render/math?math=S_n > n"> in practice.

Effeciency: <img src="https://render.githubusercontent.com/render/math?math=E_n = S_n / n">

An ideal parallel program has its effencieny equal to 1.

Scalability: high efficiency even with large processor numbers
. Kind of like
<img src="https://render.githubusercontent.com/render/math?math=E_\infty">
